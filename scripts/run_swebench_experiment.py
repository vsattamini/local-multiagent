#!/usr/bin/env python3
"""
Run SWE-bench Lite Phase 3 experiments with real model inference.

This generates patches for SWE-bench issues using the swarm system with different routing strategies.

Usage:
    # Mock mode (testing infrastructure)
    python scripts/run_swebench_experiment.py --condition experimental --n-issues 3 --mock

    # Real mode (with model inference)
    python scripts/run_swebench_experiment.py --condition experimental --n-issues 5
"""

import argparse
import json
import logging
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
import sys

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.models.llama_cpp import LlamaCppModel
from src.swarm.agent import SwarmAgent
from src.swarm.router import AffinityRouter, RandomRouter
from src.swarm.types import TaskType
from src.swebench.validation import validate_patch
from src.swebench.prompts import build_swebench_prompt, get_prompt_info, list_versions

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_curated_issues(curated_file: str, n_issues: int = None) -> List[Dict]:
    """Load curated SWE-bench issues."""
    logger.info(f"Loading curated issues from {curated_file}...")

    with open(curated_file) as f:
        data = json.load(f)

    issues = data["issues"]
    if n_issues:
        issues = issues[:n_issues]

    logger.info(f"Loaded {len(issues)} issues")
    return issues


def generate_mock_solution(issue: Dict, condition: str) -> str:
    """Generate a mock solution for testing infrastructure without model."""
    return f"""# Mock solution for {issue['instance_id']}
# Condition: {condition}
# Category: {issue.get('category', 'unknown')}

def fix_issue():
    # This is a placeholder solution
    # In a real run, this would be generated by the model
    pass
"""


def categorize_issue_to_task_type(issue: Dict) -> TaskType:
    """
    Map SWE-bench issue categories to TaskType for routing.

    This allows the router to track specialization patterns.
    """
    category = issue.get('category', 'other')
    repo = issue.get('repo', '').lower()

    # Map categories to task types based on similarity
    # bug/test -> LOGIC (reasoning about correctness)
    # feature -> STRING (often UI/string manipulation, especially in django)
    # refactor/docs -> LIST (data structure reorganization)
    # other -> MATH (catch-all for numeric/algorithmic work)

    if category in ['bug', 'test']:
        return TaskType.LOGIC
    elif category == 'feature':
        return TaskType.STRING
    elif category in ['refactor', 'docs']:
        return TaskType.LIST
    else:
        return TaskType.MATH


def generate_real_solution(
    issue: Dict,
    condition: str,
    model: Optional[LlamaCppModel],
    agents: List[SwarmAgent],
    router: Any,
    prompt_version: str = "v1"
) -> str:
    """
    Generate real solution using swarm system.

    Args:
        issue: SWE-bench issue
        condition: Experimental condition (baseline/control/experimental)
        model: Loaded language model
        agents: List of swarm agents
        router: Router instance

    Returns:
        Generated patch string
    """
    if model is None:
        raise ValueError("Model must be loaded for real solution generation")

    # Determine task type for routing
    task_type = categorize_issue_to_task_type(issue)

    # Select agent based on condition
    if condition == "baseline":
        # Single agent (always use agent 0)
        agent = agents[0]
    else:
        # Multi-agent with routing
        agent = router.select_agent(agents, task_type)

    logger.info(f"  Selected Agent {agent.agent_id} for {issue['category']} issue (task_type={task_type.value})")

    # Get context examples from agent's memory (for experimental condition)
    context_examples = None
    if condition == "experimental" and len(agent.context_buffer) > 0:
        # Filter context buffer for examples of this task type
        context_examples = [ex for ex in agent.context_buffer if ex.task_type == task_type]
        if context_examples:
            logger.info(f"  Using {len(context_examples)} context examples from agent's memory")

    # Build prompt using specified version
    prompt = build_swebench_prompt(issue, version=prompt_version, context_examples=context_examples)
    logger.info(f"  Using prompt version: {prompt_version}")

    # Generate solution
    logger.info(f"  Generating solution with {model.model_name}...")
    try:
        solution = model.generate(
            prompt,
            max_tokens=2048,
            temperature=0.2,
            stop=["<|im_end|>", "<|endoftext|>"]  # Use model's native stop tokens
        )

        # Clean up and format solution
        solution = solution.strip()
        
        # Remove markdown code blocks if present
        if solution.startswith("```"):
            lines = solution.split('\n')
            solution = '\n'.join(lines[1:-1] if lines[-1].strip() == '```' else lines[1:])
        
        # V2 prompt ends with "diff --git" priming - prepend if needed
        # V1/V3 prompts don't have this issue
        if prompt_version == "v2" and not solution.startswith("diff --git"):
            solution = "diff --git" + solution
        
        # Clean up any trailing explanation after the patch
        # Look for common markers that indicate end of patch
        end_markers = ["\n\n## ", "\n\nExplanation:", "\n\nNote:", "\n\nThis patch"]
        for marker in end_markers:
            if marker in solution:
                solution = solution.split(marker)[0]
        
        return solution.strip()

    except Exception as e:
        logger.error(f"  Error generating solution: {e}")
        # Return a minimal patch as fallback
        return f"# Error generating solution: {e}\n# Issue: {issue['instance_id']}"


def run_experiment(
    condition: str,
    curated_file: str,
    n_issues: int = None,
    output_dir: str = "results/swebench",
    model_path: str = "models/qwen2.5-coder-1.5b-instruct-q4_k_m.gguf",
    use_mock: bool = False,
    n_agents: int = 3,
    router_temp: float = 0.5,
    prompt_version: str = "v1"
):
    """
    Run SWE-bench experiment with swarm system.

    Args:
        condition: Experimental condition (baseline/control/experimental)
        curated_file: Path to curated issues JSON
        n_issues: Number of issues to process (None = all)
        output_dir: Output directory
        model_path: Path to model weights
        use_mock: If True, use mock generation (for testing infrastructure)
    """
    logger.info(f"\n{'#'*80}")
    logger.info(f"STARTING SWE-BENCH EXPERIMENT: {condition.upper()}")
    logger.info(f"Mode: {'MOCK (Infrastructure Test)' if use_mock else 'REAL (Model Inference)'}")
    logger.info(f"Prompt Version: {prompt_version}")
    logger.info(f"{'#'*80}\n")

    # Create output directory
    output_path = Path(output_dir) / condition
    output_path.mkdir(parents=True, exist_ok=True)

    # Load existing results for resume capability
    results_file = output_path / "results.json"
    existing_results = []
    processed_ids = set()
    
    if results_file.exists():
        try:
            with open(results_file) as f:
                data = json.load(f)
            existing_results = data.get("results", [])
            processed_ids = {r["instance_id"] for r in existing_results}
            if processed_ids:
                logger.info(f"Resuming: Found {len(processed_ids)} already-processed issues")
        except Exception as e:
            logger.warning(f"Could not load existing results: {e}")

    # Load issues
    all_issues = load_curated_issues(curated_file, n_issues)
    
    # Filter out already-processed issues
    issues = [issue for issue in all_issues if issue["instance_id"] not in processed_ids]
    
    if len(issues) < len(all_issues):
        logger.info(f"Skipping {len(all_issues) - len(issues)} already-processed issues")
    
    if not issues:
        logger.info("All issues already processed. Nothing to do.")
        return

    # Initialize swarm system (unless using mock)
    model = None
    agents = []
    router = None

    if not use_mock:
        logger.info("Initializing swarm system...")

        # Load model
        logger.info(f"Loading model from {model_path}...")
        model = LlamaCppModel(
            model_name="qwen2.5-coder-1.5b",
            model_path=model_path,
            n_ctx=8192,
            n_gpu_layers=-1  # Use GPU if available
        )
        model.load()

        # Initialize agents
        if condition == "baseline":
            actual_n_agents = 1
        else:
            actual_n_agents = n_agents
        agents = [
            SwarmAgent(
                agent_id=i,
                max_context_examples=5
            )
            for i in range(actual_n_agents)
        ]
        logger.info(f"Initialized {actual_n_agents} agent(s)")

        # Initialize router
        if condition == "baseline":
            router = None  # Single agent, no routing needed
        elif condition == "control":
            router = RandomRouter()
            logger.info("Using random routing (control)")
        elif condition == "experimental":
            router = AffinityRouter(temperature=router_temp)
            logger.info(f"Using affinity routing (experimental) with temp={router_temp}")

    # Process each issue (start with existing results)
    results = list(existing_results)
    start_time = time.time()


    for i, issue in enumerate(issues, 1):
        logger.info(f"\n[{i}/{len(issues)}] Processing {issue['instance_id']}")
        logger.info(f"  Repo: {issue['repo']}")
        logger.info(f"  Category: {issue.get('category', 'unknown')}")
        logger.info(f"  Problem: {issue['problem_statement'][:100]}...")

        issue_start = time.time()

        try:
            # Generate solution
            if use_mock:
                solution = generate_mock_solution(issue, condition)
                agent_id = 0
            else:
                solution = generate_real_solution(issue, condition, model, agents, router, prompt_version)
                # Track which agent was used
                task_type = categorize_issue_to_task_type(issue)
                if condition == "baseline":
                    agent_id = 0
                else:
                    agent = router.select_agent(agents, task_type)
                    agent_id = agent.agent_id

            issue_time = time.time() - issue_start

            # Validate the generated patch
            validation = validate_patch(solution)
            
            result = {
                "instance_id": issue["instance_id"],
                "repo": issue["repo"],
                "category": issue.get("category"),
                "condition": condition,
                # Clarified metrics:
                "generated": True,                    # Generation succeeded (no exception)
                "valid_patch": validation.is_valid,   # Patch looks valid (syntax + format)
                "agent_id": agent_id,
                "task_type": categorize_issue_to_task_type(issue).value,
                "execution_time": issue_time,
                "model_output": solution,
                "validation": {
                    "is_valid_syntax": validation.is_valid_syntax,
                    "is_valid_format": validation.is_valid_format,
                    "has_diff_header": validation.has_diff_header,
                    "has_hunks": validation.has_hunks,
                    "files_touched": validation.files_touched,
                    "lines_added": validation.lines_added,
                    "lines_removed": validation.lines_removed,
                    "error_message": validation.error_message
                },
                "metadata": {
                    "lines_changed": issue.get("estimated_lines_changed"),
                    "files_changed": issue.get("estimated_files_changed"),
                    "solution_length": len(solution)
                }
            }
            
            # Backwards compatibility
            result["success"] = result["generated"]
            result["solution_generated"] = result["generated"]

            status = "✓" if validation.is_valid else "⚠"
            logger.info(f"  {status} Generated by Agent {agent_id} ({issue_time:.2f}s, {len(solution)} chars, valid={validation.is_valid})")

            # Update agent context (for experimental condition)
            if not use_mock and condition == "experimental":
                task_type = categorize_issue_to_task_type(issue)
                agent = agents[agent_id]
                # Add to agent's context buffer
                agent.add_success(
                    problem=issue['problem_statement'][:200],
                    solution=solution[:200],
                    task_type=task_type
                )

        except Exception as e:
            logger.error(f"  ✗ Failed: {e}")
            result = {
                "instance_id": issue["instance_id"],
                "repo": issue["repo"],
                "category": issue.get("category"),
                "condition": condition,
                "success": False,
                "error": str(e),
                "execution_time": time.time() - issue_start
            }

        results.append(result)

        # Save intermediate results every 5 issues
        if i % 5 == 0:
            save_results(results, output_path, suffix="_partial")

    # Save final results
    save_results(results, output_path)

    # Compute summary
    total_time = time.time() - start_time
    summary = compute_summary(results, total_time)

    # Print summary
    print_summary(condition, summary, output_path)

    # Cleanup
    if model is not None:
        logger.info("\nUnloading model...")
        model.unload()

    return results, summary


def save_results(results: List[Dict], output_path: Path, suffix: str = ""):
    """Save results to JSON."""
    filename = f"results{suffix}.json"
    output_file = output_path / filename

    with open(output_file, 'w') as f:
        json.dump({
            "results": results,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }, f, indent=2)


def compute_summary(results: List[Dict], total_time: float) -> Dict:
    """Compute summary statistics."""
    total = len(results)
    successful = sum(1 for r in results if r.get("success", False))
    generated = sum(1 for r in results if r.get("solution_generated", False))
    avg_time = sum(r.get("execution_time", 0) for r in results) / total if total > 0 else 0

    # Category breakdown
    categories = {}
    for r in results:
        cat = r.get("category", "unknown")
        if cat not in categories:
            categories[cat] = {"total": 0, "success": 0}
        categories[cat]["total"] += 1
        if r.get("success", False):
            categories[cat]["success"] += 1

    # Repository breakdown
    repos = {}
    for r in results:
        repo = r.get("repo", "unknown").split('/')[-1]
        if repo not in repos:
            repos[repo] = {"total": 0, "success": 0}
        repos[repo]["total"] += 1
        if r.get("success", False):
            repos[repo]["success"] += 1

    return {
        "total": total,
        "successful": successful,
        "generated": generated,
        "success_rate": successful / total if total > 0 else 0,
        "avg_time_per_issue": avg_time,
        "total_time": total_time,
        "categories": categories,
        "repositories": repos
    }


def print_summary(condition: str, summary: Dict, output_path: Path):
    """Print experiment summary."""
    logger.info(f"\n{'#'*80}")
    logger.info(f"EXPERIMENT COMPLETE: {condition.upper()}")
    logger.info(f"{'#'*80}")

    logger.info(f"\nResults saved to: {output_path}")

    logger.info(f"\nPerformance:")
    logger.info(f"  Total issues: {summary['total']}")
    logger.info(f"  Solutions generated: {summary['generated']}")
    logger.info(f"  Successful: {summary['successful']}")
    logger.info(f"  Success rate: {summary['success_rate']:.1%}")

    logger.info(f"\nTiming:")
    logger.info(f"  Total time: {summary['total_time']:.1f}s")
    logger.info(f"  Avg per issue: {summary['avg_time_per_issue']:.1f}s")

    logger.info(f"\nCategory Distribution:")
    for cat, stats in sorted(summary['categories'].items()):
        logger.info(f"  {cat}: {stats['success']}/{stats['total']} "
                   f"({stats['success']/stats['total']:.1%})")

    logger.info(f"\nRepository Distribution:")
    for repo, stats in sorted(summary['repositories'].items()):
        logger.info(f"  {repo}: {stats['success']}/{stats['total']} "
                   f"({stats['success']/stats['total']:.1%})")


def main():
    parser = argparse.ArgumentParser(
        description="Run SWE-bench Phase 3 experiment with swarm system",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Test infrastructure with mock generation (3 issues)
  python scripts/run_swebench_experiment.py --condition experimental --n-issues 3 --mock

  # Small pilot with real model (5 issues)
  python scripts/run_swebench_experiment.py --condition experimental --n-issues 5

  # Full experiment (all 50 issues)
  python scripts/run_swebench_experiment.py --condition experimental

  # Use different model
  python scripts/run_swebench_experiment.py --condition baseline --model-path models/custom-model.gguf
        """
    )
    parser.add_argument(
        "--condition",
        type=str,
        required=True,
        choices=["baseline", "control", "experimental"],
        help="Experimental condition (baseline=1 agent, control=random routing, experimental=affinity routing)"
    )
    parser.add_argument(
        "--n-issues",
        type=int,
        default=None,
        help="Number of issues to run (default: all 50 curated issues)"
    )
    parser.add_argument(
        "--curated-file",
        type=str,
        default="data/swebench_curated.json",
        help="Path to curated issues file"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="results/swebench",
        help="Output directory for results"
    )
    parser.add_argument(
        "--model-path",
        type=str,
        default="models/qwen2.5-coder-1.5b-instruct-q4_k_m.gguf",
        help="Path to model weights (GGUF format)"
    )
    parser.add_argument(
        "--mock",
        action="store_true",
        help="Use mock generation for testing infrastructure (no model loading)"
    )
    parser.add_argument(
        "--n-agents",
        type=int,
        default=3,
        help="Number of agents for multi-agent conditions (default: 3)"
    )
    parser.add_argument(
        "--router-temp",
        type=float,
        default=0.5,
        help="Router temperature for affinity routing (default: 0.5, lower = more exploitation)"
    )
    parser.add_argument(
        "--prompt-version",
        type=str,
        default="v1",
        choices=[f"v{i}" for i in range(1, 16)],
        help="Prompt version to use: v1 (simple), v2 (few-shot with bug), v3 (fixed few-shot)"
    )

    args = parser.parse_args()

    # Validate model path exists (unless using mock)
    if not args.mock and not Path(args.model_path).exists():
        logger.error(f"Model file not found: {args.model_path}")
        logger.error("Either provide a valid --model-path or use --mock for testing")
        sys.exit(1)

    # Log prompt version info
    prompt_info = get_prompt_info(args.prompt_version)
    logger.info(f"Prompt: {prompt_info.version} - {prompt_info.description}")

    # Run experiment
    run_experiment(
        condition=args.condition,
        curated_file=args.curated_file,
        n_issues=args.n_issues,
        output_dir=args.output_dir,
        model_path=args.model_path,
        use_mock=args.mock,
        n_agents=args.n_agents,
        router_temp=args.router_temp,
        prompt_version=args.prompt_version
    )


if __name__ == "__main__":
    main()
